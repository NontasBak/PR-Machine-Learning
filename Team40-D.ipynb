{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Φωτιάδης Κωνσταντίνος (ΑΕΜ: 10726) - Μπακούλας Επαμεινώνδας (ΑΕΜ: 10683)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART D\n",
    "\n",
    "We will first install the necessary dependencies and import them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.calibration import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') # When training the models, we get some convergence warnings. We can ignore them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also import the train and test datasets. After importing, we transform the values of the features using StandardScaler to ensure that the model is not biased towards any feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('datasetTV.csv', header=None)\n",
    "test_data = pd.read_csv('datasetTest.csv', header=None)\n",
    "\n",
    "X_train = train_data.iloc[:, :-1].values # All columns except the last one\n",
    "y_train = train_data.iloc[:, -1].values # Last column is the target\n",
    "X_test = test_data.values\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training our models, we will do some hyperparameter tuning to find the best hyperparameters for our models. We will use GridSearchCV for this purpose. It performs an exhaustive search over specified parameter values for an estimator. The accuracy of the model is calculated using 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample hyperparameters for each model\n",
    "param_grids = {\n",
    "    'k-NN': {\n",
    "        'n_neighbors': [3, 5, 7, 9, 13, 17],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'p': [1, 2]\n",
    "    },\n",
    "    'Perceptron': {\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        'alpha': [1e-4, 1e-3, 1e-2],\n",
    "        'max_iter': [1000, 2000]\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10, 100, 1000],\n",
    "        'kernel': ['poly', 'rbf', 'sigmoid'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'SVM Linear': {\n",
    "        'C': [0.1, 1, 10, 100]\n",
    "    },\n",
    "    'Random Forests': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 1.0]\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Neural Networks': {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (100, 50)],\n",
    "        'activation': ['relu', 'tanh', 'logistic'],\n",
    "        'alpha': [1e-5, 1e-4],\n",
    "        'learning_rate_init': [1e-3, 1e-4]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform Grid Search for each model\n",
    "results = {}\n",
    "\n",
    "# Naive Bayes model (has no hyperparameters to tune)\n",
    "naive_bayes_model = GaussianNB()\n",
    "nb_scores = cross_val_score(naive_bayes_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "results['Naive Bayes'] = {\n",
    "    'Best Parameters': None,\n",
    "    'Best Accuracy': np.mean(nb_scores)\n",
    "}\n",
    "\n",
    "for model_name, param_grid in param_grids.items():\n",
    "    print(f\"Tuning {model_name}...\")\n",
    "    model = {\n",
    "        'k-NN': KNeighborsClassifier(),\n",
    "        'Perceptron': Perceptron(),\n",
    "        'Logistic Regression': LogisticRegression(max_iter=500),\n",
    "        'SVM': SVC(),\n",
    "        'SVM Linear': LinearSVC(),\n",
    "        'Random Forests': RandomForestClassifier(random_state=42),\n",
    "        'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "        'Neural Networks': MLPClassifier(max_iter=500, random_state=42)\n",
    "    }.get(model_name)\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    results[model_name] = {\n",
    "        'Best Parameters': grid_search.best_params_,\n",
    "        'Best Accuracy': grid_search.best_score_\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above step took a total 1.5 hours, which is worth noting. Now we can print the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: Best Parameters = None\n",
      "Naive Bayes: Best Accuracy = 0.7026\n",
      "k-NN: Best Parameters = {'n_neighbors': 9, 'p': 2, 'weights': 'distance'}\n",
      "k-NN: Best Accuracy = 0.8220\n",
      "Perceptron: Best Parameters = {'alpha': 0.0001, 'max_iter': 1000, 'penalty': 'l1'}\n",
      "Perceptron: Best Accuracy = 0.7057\n",
      "Logistic Regression: Best Parameters = {'C': 0.01, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Logistic Regression: Best Accuracy = 0.7811\n",
      "SVM: Best Parameters = {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "SVM: Best Accuracy = 0.8534\n",
      "SVM Linear: Best Parameters = {'C': 0.1}\n",
      "SVM Linear: Best Accuracy = 0.7634\n",
      "Random Forests: Best Parameters = {'max_depth': 20, 'max_features': 'sqrt', 'n_estimators': 200}\n",
      "Random Forests: Best Accuracy = 0.8140\n",
      "AdaBoost: Best Parameters = {'learning_rate': 1.0, 'n_estimators': 200}\n",
      "AdaBoost: Best Accuracy = 0.6609\n",
      "Decision Tree: Best Parameters = {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "Decision Tree: Best Accuracy = 0.6390\n",
      "Neural Networks: Best Parameters = {'activation': 'relu', 'alpha': 1e-05, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001}\n",
      "Neural Networks: Best Accuracy = 0.8279\n"
     ]
    }
   ],
   "source": [
    "for model_name, result in results.items():\n",
    "    print(f\"{model_name}: Best Parameters = {result['Best Parameters']}\")\n",
    "    print(f\"{model_name}: Best Accuracy = {result['Best Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will select the model with the best accuracy, using the optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: SVM with accuracy 0.8534\n",
      "Hyperparameters of the best model: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# Print the best model\n",
    "best_model = max(results, key=lambda key: results[key]['Best Accuracy'])\n",
    "best_accuracy = results[best_model]['Best Accuracy']\n",
    "print(f\"Best Model: {best_model} with accuracy {best_accuracy:.4f}\")\n",
    "print(f\"Hyperparameters of the best model: {results[best_model]['Best Parameters']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with the highest accuracy is **SVM (Support Vector Machine)** with hyperparameters shown above. We will use this model to make predictions on the test dataset. First, we define this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best_model = SVC(kernel='rbf', C=10, gamma='auto', random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to train the model on the entire dataset, and then make predictions on the test dataset. We will then save the predictions to a numpy file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best_model.fit(X_train, y_train)\n",
    "y_test = clf_best_model.predict(X_test)\n",
    "\n",
    "np.save('labels40.npy', y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
